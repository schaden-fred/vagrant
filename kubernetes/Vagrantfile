 servers=[
      {
        :hostname => "master",
        :ip => "192.168.100.10",
		:box => "centos/7",
        :ram => 2048,
        :cpu => 2
      },
      {
        :hostname => "node1",
        :ip => "192.168.100.11",
        :box => "centos/7",
        :ram => 2048,
        :cpu => 2
      },
	  {
        :hostname => "node2",
        :ip => "192.168.100.12",
        :box => "centos/7",
        :ram => 2048,
        :cpu => 2
      }
    ]


Vagrant.configure(2) do |config|
    servers.each do |machine|
        config.vm.define machine[:hostname] do |node|
            node.vm.box = machine[:box]
            node.vm.hostname = machine[:hostname]
            node.vm.network "private_network", ip: machine[:ip]
			#,virtualbox__intnet: "kubernetes_cluster"
			node.vm.provider "virtualbox" do |vb|
                vb.customize ["modifyvm", :id, "--memory", machine[:ram]]
				vb.customize ["modifyvm", :id, "--cpus", machine[:cpu]]
			#	vb.customize ["modifyvm", :id, "--nic1", "natnetwork"]
			#	vb.customize ["modifyvm", :id, "--nat-network1", "KubernetesNatNetwork"]
			end
		end
		#Run at this level, the shell script will iterate through each value in var servers,
		#and write a hosts file entry for each one.
		config.vm.provision :shell, :path => 'setup.sh', :args => [machine[:ip], machine[:hostname]]
		# Don't put any other shell commands at this level unless you want them to run multiple times.
	end
	
	# Shell commands at this level will only run once.
	#config.vm.provision "shell", inline: "yum update -y"
	#disable SELinux for simplicity.  Don't do this in prod.
	config.vm.provision "shell", inline: "setenforce 0"
	config.vm.provision "shell", inline: "sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux"
	#Enable the br_netfilter module for cluster communication.
	config.vm.provision "shell", inline: "modprobe br_netfilter"
	#Allow Kubernetes to manipulate iptables
	config.vm.provision "shell", inline: "echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables"
	#Disable swap to prevent memory allocation issues.
	config.vm.provision "shell", inline: "swapoff -a"
	config.vm.provision "shell", inline: "sed -e '/swap/ s/^#*/#/' -i /etc/fstab"
	#Install docker prerequisites
	config.vm.provision "shell", inline: "yum install -y yum-utils device-mapper-persistent-data lvm2"
	#Add the Docker repo and install Docker.
	config.vm.provision "shell", inline: "yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo"
	config.vm.provision "shell", inline: "yum install -y docker-ce"
	#Configure the Docker Cgroup Driver to systemd, enable and start Docker
	config.vm.provision "shell", inline: "sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service"
	config.vm.provision "shell", inline: "systemctl daemon-reload"
	config.vm.provision "shell", inline: "systemctl enable docker --now"
	config.vm.provision "shell", inline: "systemctl status docker"
	config.vm.provision "shell", inline: "docker info | grep -i cgroup"
	#add the kubernetes repo
	config.vm.provision "file", source: "kubernetes.repo", destination: "~vagrant/kubernetes.repo"
	config.vm.provision "shell", inline: "sudo mv ~vagrant/kubernetes.repo /etc/yum.repos.d/kubernetes.repo"
	#Install Kubernetes
	config.vm.provision "shell", inline: "yum install -y kubelet kubeadm kubectl"
	#Enable Kubernetes. The kubelet service will not start until you run kubeadm init on the master node, or join the worker node to the master.
	config.vm.provision "shell", inline: "systemctl enable kubelet"
	#Prevent an error on preflight due to container runtime not running.
	config.vm.provision "shell", inline: "rm /etc/containerd/config.toml"
	config.vm.provision "shell", inline: "systemctl restart containerd"
	config.vm.provision "shell", inline: "echo ================HELLO FROM $HOSTNAME.  READY TO WORK================"
end	


# #Master specific commands go here
# vagrant.configure("2") do |config|
	# config.vm.define "master" do |master|
		# #Initialize the cluster using the IP range for Flannel.
		# #IP range will be 10.244.0.0/16 to work with Flannel.  Make sure that nothing in your network conflicts with this.
		# Since this is on a vagrant private network, there's no chance of conflict unless you alter the
		# IPs range 10.244.0.0 - 10.244.255.255
		# master.vm.provision "shell", inline: "kubeadm init --pod-network-cidr=10.244.0.0/16"
		# #Copy the kubeadm join command from the output above
		# master.vm.provision "shell", inline: "mkdir -p ~vagrant/.kube"
		# master.vm.provision "shell", inline: "sudo cp -i /etc/kubernetes/admin.conf ~vagrant/.kube/config"
		# master.vm.provision "shell", inline: "sudo chown $(id -u):$(id -g) ~vagrant/.kube/config"
		# #Deploy Flannel.
		# master.vm.provision "shell", inline: "kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml"
		# #Check the cluster state.
		# master.vm.provision "shell", inline: "kubectl get pods --all-namespaces"
		# 
	# end
# end	

	#Note: Complete the following steps on the NODES ONLY!
	#Run the join command that you copied earlier (this command needs to be run as sudo), then check your nodes from the master.
	#kubectl get nodes


	

